# -*- coding: utf-8 -*-
"""beta3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sJfC_7-b1ep8UYkspSQr85pL_VyRQfNG
"""

!pip install yfinance
!pip install talipp
!pip install ta
!pip install pandas_ta
!pip install alpaca-trade-api
!pip install vaderSentiment

import pandas as pd
import numpy as np
import yfinance as yf
import json
import urllib.request
from datetime import timedelta
from datetime import date, timedelta
import datetime
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()
import matplotlib.pyplot as plt
import seaborn as sns
import pandas_ta as pta
from ta.trend import MACD
import plotly.graph_objects as go
from numpy import array
from numpy import hstack
import requests
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

from keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM,GRU, Bidirectional
from tensorflow.keras.layers import Conv2D, MaxPool2D
from tensorflow.keras import Input
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.layers import TimeDistributed
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Flatten

stock="EURUSD=X"
start_date=date(2023,2,10)   
end_date=date(2023,2,11)

def sentiment_analyzer_scores(sentence):
    score = analyser.polarity_scores(sentence)
    return score

"""###news"""

from alpaca_trade_api import REST, Stream

API_KEY='PKG57G9MTU5BTXL1WO1Q'
API_SECRET='Y5SnEp80bKpAneesv3wo4c1dDh0k69QareCf54pW'
rest_client = REST(API_KEY, API_SECRET)
l=rest_client.get_news("{}".format(stock), "{}".format(str(start_date)), "{}".format(str(end_date)),limit=10000)

len(l)

d=pd.DataFrame(index=np.arange(10000), columns=['date','headline','source'])
date_list=[]
headline_list=[]
source_list=[]
for i in range(len(l)):     
              date_list.append(l[i].created_at)
              headline_list.append(l[i].headline)
              source_list.append(l[i].source)
for i in range(len(date_list)):
              d["date"].iloc[i]=date_list[i] 
              d["headline"].iloc[i]=headline_list[i]        
              d["source"].iloc[i]=source_list[i]        
d.dropna(inplace=True)

d['date']=pd.to_datetime(d['date'])
d['date'] = d['date'].dt.strftime('%Y%m%d')
d["date"]=pd.to_datetime(d["date"])
d.sort_values(["date"],inplace=True)
d.reset_index(drop=True,inplace=True)
d

d["compound"]=0
for i in range(len(d)):
     d["compound"].iloc[i]=sentiment_analyzer_scores(d["headline"].iloc[i])['compound']

for ii in range(4):
      dlist=[]
      for i in range(len(d)):
        if(i<=(len(d)-2)):
    
                  if(d['date'].iloc[i]!=d["date"].iloc[i+1]):
                       dlist.append(d["compound"].iloc[i])
                       h=0
                       for ii in range(len(dlist)): 
                          h=dlist[ii]+h
                       dlist.clear()
                       d["compound"].iloc[i]=h  
                       v=d[(d['date']==d["date"].iloc[i]) & (d.index<d[i:i+1].index[0])].index
                       d.drop(v,inplace=True) 
                  else: 
                       dlist.append(d["compound"].iloc[i])  
        else:
                  d.reset_index(drop=True,inplace=True)
                  if(d['date'].iloc[-1]==d["date"].iloc[-2]):
                       dlist.append(d["compound"].iloc[-1])
                       h=0
                       for ii in range(len(dlist)): 
                          h=dlist[ii]+h  
                       dlist.clear()
                       d["compound"].iloc[-1]=h  
                       v=d[(d['date']==d["date"].iloc[-1]) & (d.index<d[-1:].index[0])].index
                       d.drop(v,inplace=True)

"""###data"""

msft = yf.Ticker(stock)
df = msft.history(start="{}".format(str(start_date)),end="{}".format(str(end_date)),interval='1D')

df['RSI']=pta.rsi(df['Close'],length=14)
df["CCI"]=pta.cci(df["High"],df["Low"],df["Close"],length=14)
df['Fisher']=pta.fisher(df['High'],df['Low'],14)['FISHERT_14_1']
df.drop(['Dividends','Stock Splits'], axis=1, inplace=True)
df.dropna(inplace=True)
df.reset_index(inplace = True)
df['Date'] = df['Date'].dt.strftime('%Y%m%d')
df["Date"]=pd.to_datetime(df["Date"])
pta.fisher(df['High'],df['Low'],14)



df['news']=0

t1=[]
for i in range(len(df)):
    for ii in range(len(d)):
       if((df["Date"].iloc[i]==d["date"].iloc[ii]) or (df["Date"].iloc[i]>d["date"].iloc[ii] and df["Date"].iloc[i-1]<d["date"].iloc[ii])):
              t1.append(d["compound"].iloc[ii])
                          
    df["news"].iloc[i]=sum(t1)
    t1.clear()

df['news']=df.news.shift(1)
df.fillna(0,inplace=True)

df.drop(['Open'],axis=1,inplace=True)

df

"""###Deep learning"""

print(np.round(len(df)*(96.6/100)))
print(np.round(len(df)*(1.7/100)))
print(np.round(len(df)*(0.7/100)))

train=df[:int(np.round(len(df)*(98.7/100)))]
valid=df[int(np.round(len(df)*(98.7/100))):]
valid.reset_index(drop=True,inplace=True)

valid.shape

def split_sequences(sequences, n_steps,X=[],y=[]):
	for i in range(len(sequences)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the dataset
		if end_ix+1 > len(sequences):
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequences[i:end_ix,:], sequences[end_ix,0:3]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
 
# define input sequence
for i in range(len(train.columns)):
	         globals()['in%s' % i]=array(train[train.columns[i]]).reshape(-1,1)
	         

	    

#dataset = hstack((in1,in2,in3,in4,in6,in7,in8,in9,in10,in11,in12,in13,in14,in15,in16,in17,in18,in19,in20))
dataset = hstack((in1,in2,in3,in5,in6,in7,in8))




n_steps = 10
X, y = split_sequences(dataset, n_steps)
print(X.shape, y.shape)
# summarize the data
for i in range(10):
	print(X[i], y[i])

u=pd.DataFrame(index=np.arange(len(valid)), columns=['High', 'Low', 'Close'])
valid1_GRU=pd.concat([train,u], axis=0)
valid2_GRU=pd.concat([train,u], axis=0)
valid3_GRU=pd.concat([train,u], axis=0)
valid4_GRU=pd.concat([train,u], axis=0)
valid5_GRU=pd.concat([train,u], axis=0)
valid6_GRU=pd.concat([train,u], axis=0)
valid7_GRU=pd.concat([train,u], axis=0)
valid8_GRU=pd.concat([train,u], axis=0)
valid9_GRU=pd.concat([train,u], axis=0)
valid10_GRU=pd.concat([train,u], axis=0)
valid1_GRU.reset_index(drop=True,inplace=True)
valid2_GRU.reset_index(drop=True,inplace=True)
valid3_GRU.reset_index(drop=True,inplace=True)
valid4_GRU.reset_index(drop=True,inplace=True)
valid5_GRU.reset_index(drop=True,inplace=True)
valid6_GRU.reset_index(drop=True,inplace=True)
valid7_GRU.reset_index(drop=True,inplace=True)
valid8_GRU.reset_index(drop=True,inplace=True)
valid9_GRU.reset_index(drop=True,inplace=True)
valid10_GRU.reset_index(drop=True,inplace=True)

def TZ_GRU(xx,yy,step,features):
   epochs = 50
   batch_size = 32
   model = Sequential()
   model.add(GRU(32, activation="softplus" ,return_sequences=True, input_shape=(step, features)))#softplus-selu-relu-elu
   model.add(GRU(32, activation="softplus" , return_sequences=False))

   
   
  
   model.add(Dense(y.shape[1]))
   model.compile(optimizer='RMSprop', loss='mse')#RMSprop-adam
   model.fit(X, y,epochs=epochs,verbose=0,batch_size=batch_size)
   
   return model

TZ_GRU1=TZ_GRU(X,y,n_steps,X.shape[2])
TZ_GRU2=TZ_GRU(X,y,n_steps,X.shape[2])
TZ_GRU3=TZ_GRU(X,y,n_steps,X.shape[2])
TZ_GRU4=TZ_GRU(X,y,n_steps,X.shape[2])
TZ_GRU5=TZ_GRU(X,y,n_steps,X.shape[2])
TZ_GRU6=TZ_GRU(X,y,n_steps,X.shape[2])
TZ_GRU7=TZ_GRU(X,y,n_steps,X.shape[2])
TZ_GRU8=TZ_GRU(X,y,n_steps,X.shape[2])
TZ_GRU9=TZ_GRU(X,y,n_steps,X.shape[2])
TZ_GRU10=TZ_GRU(X,y,n_steps,X.shape[2])

n_features = X.shape[2]

def TZERO(xx,p_model,r):      
      n=xx[xx["Close"].isnull()].index[0]
      list2=[]
      for i in range(r):


             x_input_High=[]
             x_input_Low=[]
             #x_input_Open=[]
             x_input_Close=[]
             x_input_RSI=[]
             x_input_CCI=[]
             x_input_Fish=[]
        
             x_input_news=[]

       

             for ii in range(n_steps):
            
        
               x_input_news.append(xx["news"].iloc[n-(ii+1)])
              
             

               x_input_High.append(xx["High"].iloc[n-(ii+1)])
               x_input_Low.append(xx["Low"].iloc[n-(ii+1)])
               #x_input_Open.append(xx["Open"].iloc[n-(ii+1)])
               x_input_Close.append(xx["Close"].iloc[n-(ii+1)])
               x_input_RSI.append(xx["RSI"].iloc[n-(ii+1)])
               x_input_CCI.append(xx["CCI"].iloc[n-(ii+1)])
               x_input_Fish.append(xx["Fisher"].iloc[n-(ii+1)])

        
               
            
             x_input_news=array(x_input_news).reshape(-1,1)
           
         

             x_input_High=array(x_input_High).reshape(-1,1)
             x_input_Low=array(x_input_Low).reshape(-1,1)
             #x_input_Open=array(x_input_Open).reshape(-1,1)
             x_input_Close=array(x_input_Close).reshape(-1,1)
             x_input_RSI=array(x_input_RSI).reshape(-1,1)
             x_input_CCI=array(x_input_CCI).reshape(-1,1)
             x_input_Fish=array(x_input_Fish).reshape(-1,1)


           
        
        

             merge_input = hstack((
                                   #x_input_Open,
                                   x_input_High,
                                   x_input_Low,
                                   x_input_Close,

                                   x_input_RSI,
                                   x_input_CCI,
                                   x_input_Fish,
                                   x_input_news
                                
                                   ))
             merge_input = merge_input.reshape((1, n_steps, n_features))
             yhat = p_model.predict(merge_input, verbose=0)
             #xx["Open"].iloc[n]=yhat[0][0]
             xx["High"].iloc[n]=yhat[0][0]
             xx["Low"].iloc[n]=yhat[0][1]
             xx["Close"].iloc[n]=yhat[0][2]
           
       

             xx[n:n+1]['RSI']=pta.rsi(xx[:n+1]["Close"], length=14)[n:n+1]
             xx[n:n+1]['CCI']=pta.cci(xx[:n+1]["High"],xx[:n+1]["Low"],xx[:n+1]["Close"], length=24)[n:n+1]
             xx[n:n+1]['Fisher']=pta.fisher(xx[:n+1]['High'],xx[:n+1]['Low'],14)['FISHERT_14_1'][n:n+1]


             
             xx['news'][n:n+1]=0
             



      
             n+=1

TZERO(valid1_GRU,TZ_GRU1,len(valid))
TZERO(valid2_GRU,TZ_GRU2,len(valid))
TZERO(valid3_GRU,TZ_GRU3,len(valid))
TZERO(valid4_GRU,TZ_GRU4,len(valid))
TZERO(valid5_GRU,TZ_GRU5,len(valid))
TZERO(valid6_GRU,TZ_GRU6,len(valid))
TZERO(valid7_GRU,TZ_GRU7,len(valid))
TZERO(valid8_GRU,TZ_GRU8,len(valid))
TZERO(valid9_GRU,TZ_GRU9,len(valid))
TZERO(valid10_GRU,TZ_GRU10,len(valid))
##################################

train=pd.concat([train,valid],axis=0)
train.reset_index(drop=True,inplace=True)

u=pd.DataFrame(index=np.arange(len(valid)), columns=['Open', 'High', 'Low', 'Close'])
test1_GRU=pd.concat([train,u], axis=0)
test2_GRU=pd.concat([train,u], axis=0)
test3_GRU=pd.concat([train,u], axis=0)
test4_GRU=pd.concat([train,u], axis=0)
test5_GRU=pd.concat([train,u], axis=0)
test6_GRU=pd.concat([train,u], axis=0)
test7_GRU=pd.concat([train,u], axis=0)
test8_GRU=pd.concat([train,u], axis=0)
test9_GRU=pd.concat([train,u], axis=0)
test10_GRU=pd.concat([train,u], axis=0)
test1_GRU.reset_index(drop=True,inplace=True)
test2_GRU.reset_index(drop=True,inplace=True)
test3_GRU.reset_index(drop=True,inplace=True)
test4_GRU.reset_index(drop=True,inplace=True)
test5_GRU.reset_index(drop=True,inplace=True)
test6_GRU.reset_index(drop=True,inplace=True)
test7_GRU.reset_index(drop=True,inplace=True)
test8_GRU.reset_index(drop=True,inplace=True)
test9_GRU.reset_index(drop=True,inplace=True)
test10_GRU.reset_index(drop=True,inplace=True)
################################################
TZERO(test1_GRU,TZ_GRU1,len(valid))
TZERO(test2_GRU,TZ_GRU2,len(valid))
TZERO(test3_GRU,TZ_GRU3,len(valid))
TZERO(test4_GRU,TZ_GRU4,len(valid))
TZERO(test5_GRU,TZ_GRU5,len(valid))
TZERO(test6_GRU,TZ_GRU6,len(valid))
TZERO(test7_GRU,TZ_GRU7,len(valid))
TZERO(test8_GRU,TZ_GRU8,len(valid))
TZERO(test9_GRU,TZ_GRU9,len(valid))
TZERO(test10_GRU,TZ_GRU10,len(valid))

############META LEARNER################

from xgboost import XGBRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LassoCV
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import StackingRegressor
from sklearn.ensemble import VotingRegressor
from sklearn.ensemble import HistGradientBoostingRegressor


from sklearn.linear_model import RidgeCV
from sklearn.svm import LinearSVR
estimators = [
    ('lr', RidgeCV()),
    ('svr', LinearSVR(random_state=42))
]


#'valid1_LSTM', 'valid2_LSTM', 'valid3_LSTM', 'valid4_LSTM','valid5_LSTM','valid6_LSTM','valid7_LSTM','valid8_LSTM','valid9_LSTM','valid10_LSTM',
db1=pd.DataFrame(index=np.arange(len(valid)), columns=[
                'valid1_GRU', 'valid2_GRU', 'valid3_GRU', 'valid4_GRU','valid5_GRU','valid6_GRU','valid7_GRU','valid8_GRU','valid9_GRU','valid10_GRU',
                'valid(real)'
                ])
db1["valid1_GRU"]=valid1_GRU[-len(valid):]["Close"].values
db1["valid2_GRU"]=valid2_GRU[-len(valid):]["Close"].values
db1["valid3_GRU"]=valid3_GRU[-len(valid):]["Close"].values
db1["valid4_GRU"]=valid4_GRU[-len(valid):]["Close"].values
db1["valid5_GRU"]=valid5_GRU[-len(valid):]["Close"].values
db1["valid6_GRU"]=valid6_GRU[-len(valid):]["Close"].values
db1["valid7_GRU"]=valid7_GRU[-len(valid):]["Close"].values
db1["valid8_GRU"]=valid8_GRU[-len(valid):]["Close"].values
db1["valid9_GRU"]=valid9_GRU[-len(valid):]["Close"].values
db1["valid10_GRU"]=valid10_GRU[-len(valid):]["Close"].values
db1["valid(real)"]=valid["Close"]



lcv_model=LassoCV(fit_intercept=False)
lcv_model.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])

xgb_model=XGBRegressor(booster='gblinear')
xgb_model.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])

knn_model=KNeighborsRegressor(n_neighbors=4)#n_neighbors=4
knn_model.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])

ridge_model=Ridge(fit_intercept=False,positive=True)
ridge_model.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])




rfr_model=RandomForestRegressor()#criterion='poisson'
rfr_model.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])

ada=AdaBoostRegressor(n_estimators=100, random_state=0)
ada.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])

bag = BaggingRegressor(n_estimators=100, random_state=0)
bag.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])

ext=ExtraTreesRegressor(n_estimators=100, random_state=0)
ext.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])

gbr=GradientBoostingRegressor()
gbr.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])

sr=StackingRegressor(estimators=estimators,
    final_estimator=RandomForestRegressor(n_estimators=10,
                                          random_state=42))
sr.fit(db1.drop(["valid(real)"], axis=1),db1["valid(real)"])

#'test1_LSTM', 'test2_LSTM', 'test3_LSTM', 'test4_LSTM','test5_LSTM','test6_LSTM','test7_LSTM','test8_LSTM','test9_LSTM','test10_LSTM',
db2=pd.DataFrame(index=np.arange(len(valid)), columns=[
                 'test1_GRU', 'test2_GRU', 'test3_GRU', 'test4_GRU','test5_GRU','test6_GRU','test7_GRU','test8_GRU','test9_GRU','test10_GRU'
                 ])
db2["test1_GRU"]=test1_GRU[-len(valid):]["Close"].values
db2["test2_GRU"]=test2_GRU[-len(valid):]["Close"].values
db2["test3_GRU"]=test3_GRU[-len(valid):]["Close"].values
db2["test4_GRU"]=test4_GRU[-len(valid):]["Close"].values
db2["test5_GRU"]=test5_GRU[-len(valid):]["Close"].values
db2["test6_GRU"]=test6_GRU[-len(valid):]["Close"].values
db2["test7_GRU"]=test7_GRU[-len(valid):]["Close"].values
db2["test8_GRU"]=test8_GRU[-len(valid):]["Close"].values
db2["test9_GRU"]=test9_GRU[-len(valid):]["Close"].values
db2["test10_GRU"]=test10_GRU[-len(valid):]["Close"].values



######for fix xgboost error#####
db1["valid1_GRU"][-len(valid):]=db2["test1_GRU"].values
db1["valid2_GRU"][-len(valid):]=db2["test2_GRU"].values
db1["valid3_GRU"][-len(valid):]=db2["test3_GRU"].values
db1["valid4_GRU"][-len(valid):]=db2["test4_GRU"].values
db1["valid5_GRU"][-len(valid):]=db2["test5_GRU"].values
db1["valid6_GRU"][-len(valid):]=db2["test6_GRU"].values
db1["valid7_GRU"][-len(valid):]=db2["test7_GRU"].values
db1["valid8_GRU"][-len(valid):]=db2["test8_GRU"].values
db1["valid9_GRU"][-len(valid):]=db2["test9_GRU"].values
db1["valid10_GRU"][-len(valid):]=db2["test10_GRU"].values

######for fix xgboost error#####

p1=lcv_model.predict(db2)


p2=rfr_model.predict(db2)


p3=knn_model.predict(db2)


p4=xgb_model.predict(db1.drop(["valid(real)"], axis=1))


p5=ridge_model.predict(db2)





p6=ada.predict(db2)


p7=bag.predict(db2)


p8=sr.predict(db2)


p9=ext.predict(db2)


p10=gbr.predict(db2)

fig = go.Figure(data=[go.Scatter(x=train.index, y=train["Close"],mode='lines',name='train'),
                     
                      go.Scatter(x=test1_GRU[-len(valid):].index, y=test1_GRU[-len(valid):]["Close"],mode='lines',name='test1'),
                      go.Scatter(x=test2_GRU[-len(valid):].index, y=test2_GRU[-len(valid):]["Close"],mode='lines',name='test2'),
                      go.Scatter(x=test3_GRU[-len(valid):].index, y=test3_GRU[-len(valid):]["Close"],mode='lines',name='test3'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=test4_GRU[-len(valid):]["Close"],mode='lines',name='test4'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=test5_GRU[-len(valid):]["Close"],mode='lines',name='test5'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=test6_GRU[-len(valid):]["Close"],mode='lines',name='test5'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=test7_GRU[-len(valid):]["Close"],mode='lines',name='test5'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=test8_GRU[-len(valid):]["Close"],mode='lines',name='test5'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=test9_GRU[-len(valid):]["Close"],mode='lines',name='test5'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=test10_GRU[-len(valid):]["Close"],mode='lines',name='test5'),
                    
                  
                      ]
                      ,layout=go.Layout(template="plotly_dark",
        title=go.layout.Title(text="{}".format(stock)+" Chart(GRU)")
    )
)



fig.show()

fig = go.Figure(data=[go.Scatter(x=train.index, y=train["Close"],mode='lines',name='train'),
                      

                      go.Scatter(x=test1_GRU[-len(valid):].index, y=p1,mode='lines',name='test1'),
                      go.Scatter(x=test2_GRU[-len(valid):].index, y=p2,mode='lines',name='test2'),
                      go.Scatter(x=test3_GRU[-len(valid):].index, y=p3,mode='lines',name='test3'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=p4,mode='lines',name='test4'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=p5,mode='lines',name='test5'),

                      go.Scatter(x=test4_GRU[-len(valid):].index, y=p6,mode='lines',name='test6'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=p7,mode='lines',name='test7'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=p8,mode='lines',name='test8'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=p9,mode='lines',name='test9'),
                      go.Scatter(x=test4_GRU[-len(valid):].index, y=p10,mode='lines',name='test10'),
                      
                    
                  
                      ]
                      ,layout=go.Layout(template="plotly_dark",
        title=go.layout.Title(text="{}".format(stock)+" Chart")
    )
)



fig.show()