# -*- coding: utf-8 -*-
"""c3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h_g03wqq3MBq5bH55Jhd48Roq2qN1Nh5
"""

!pip install pyvirtualdisplay
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1
!pip install gym[box2d]

from collections import deque
import numpy as np
from tensorflow.keras import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras import Model
from tensorflow.keras import optimizers
#from numpy import random
import random
import tensorflow as tf
import gym

import gym
from gym.wrappers.record_video import RecordVideo
import glob
import io
import base64
from IPython.display import HTML
from pyvirtualdisplay import Display
from IPython import display as ipythondisplay
import numpy as np

"""###Video"""

import os
os.environ["SDL_VIDEODRIVER"] = "dummy"

display = Display(visible=0, size=(1400, 900))
display.start()



def show_video():
    mp4list = glob.glob('video/*.mp4')
    if len(mp4list) > 0:
        mp4 = mp4list[0]
        video = io.open(mp4, 'r+b').read()
        encoded = base64.b64encode(video)
        ipythondisplay.display(HTML(data='''<video alt="test" autoplay 
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
    else:
        print("Could not find video")


def wrap_env(env):
    env = RecordVideo(env, './video',  episode_trigger = lambda episode_number: True)
    return env

env = wrap_env(gym.make("LunarLander-v2", render_mode="rgb_array"))

# Define hyperparameters
num_episodes = 100
max_steps_per_episode = 1000
batch_size = 64
learning_rate = 0.001
gamma = 0.99
epsilon = 1.0
epsilon_decay = 0.995
epsilon_min = 0.01
memory_size = 100000
target_update_freq = 10
optimizer=optimizers.RMSprop(learning_rate=learning_rate)
num_actions=env.action_space.n

def deep_q_network(state_shape, action_size, learning_rate, hidden_neurons):
    state_input = Input(state_shape, name='frames')
    hidden_1 = Dense(hidden_neurons, activation='relu')(state_input)
    hidden_2 = Dense(hidden_neurons, activation='relu')(hidden_1)
    q_values = Dense(action_size)(hidden_2)
    model = Model(inputs=state_input, outputs=q_values)
    model.compile(loss='mse', optimizer=optimizer)
    return model


q_network = deep_q_network(env.observation_space.shape, env.action_space.n, learning_rate,50)
target_network = deep_q_network(env.observation_space.shape, env.action_space.n, learning_rate,50)
target_network.set_weights(q_network.get_weights())
memory = []
steps = 0

def get_action(state, epsilon):
        if np.random.rand() <= epsilon:
            return np.random.randint(num_actions)
        else:
            q_values = q_network(tf.convert_to_tensor(state[None], dtype=tf.float32))
            return np.argmax(q_values[0])

def remember(state, action, reward, next_state, done):
        memory.append((state, action, reward, next_state, done))
        if len(memory) > memory_size:
            memory.pop(0)

def experience_replay():
        if len(memory) < batch_size:
          return
        
        batch = np.array(random.sample(memory, batch_size))
        states = np.array([i[0] for i in batch])
        actions = np.array([i[1] for i in batch])
        rewards = np.array([i[2] for i in batch])
        next_states = np.array([i[3] for i in batch])
        dones = np.array([i[4] for i in batch])
        
        q_next = target_network(next_states)
        q_next_max = tf.reduce_max(q_next, axis=1)
        q_target = rewards + gamma * q_next_max * (1 - dones)
        
        with tf.GradientTape() as tape:
            q_values = q_network(states)
            q_action = tf.reduce_sum(tf.one_hot(actions, num_actions) * q_values, axis=1)
            loss = tf.reduce_mean(tf.square(q_target - q_action))
        
        grads = tape.gradient(loss, q_network.trainable_variables)
        optimizer.apply_gradients(zip(grads, q_network.trainable_variables))

def update_target_network():
        target_network.set_weights(q_network.get_weights())

def train():
        scores = []
        for episode in range(num_episodes):
            print('Episode: '+str(episode))
            state = env.reset()
            score = 0
            for step in range(max_steps_per_episode):
                action=get_action(state,epsilon)
                new_state, reward, done, info =env.step(action)
                remember(state,action,reward,new_state, done)
                experience_replay() 
                update_target_network()     
train()